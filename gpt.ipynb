{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-13T12:19:29.277213Z",
     "start_time": "2025-09-13T12:19:28.211724Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "df = pd.read_json('tsar2025_test_blind.jsonl', lines =True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:19:29.857270Z",
     "start_time": "2025-09-13T12:19:29.853544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "def gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        temperature=0.1,\n",
    "        top_p = 0.2,\n",
    "        seed = 42\n",
    "    )\n",
    "\n",
    "    full_text = \"\"\n",
    "    for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.content:\n",
    "            full_text += delta.content\n",
    "\n",
    "    return full_text"
   ],
   "id": "ee632842bbcbf9ab",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T12:28:03.060409Z",
     "start_time": "2025-09-13T12:19:34.195629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_simplifications = []\n",
    "text_ids = []\n",
    "for idx in range(len(df)):\n",
    "    target_level = df['target_cefr'][idx]\n",
    "    original_text = df['original'][idx]\n",
    "    text_id = df['text_id'][idx]\n",
    "\n",
    "    if target_level == 'A2':\n",
    "        prompt = (\n",
    "    f\"You are a language teacher simplifying texts to A2 CEFR level.\\n\\n\"\n",
    "    f\"OBJECTIVE: Transform this text to A2 level while preserving all original meaning and information.\\n\\n\"\n",
    "    f\"A2 LANGUAGE REQUIREMENTS:\\n\"\n",
    "    f\"- Vocabulary: Most common 1500 English words only\\n\"\n",
    "    f\"- Sentences: 8-12 words, one clear idea per sentence\\n\"\n",
    "    f\"- Grammar: Simple present/past, basic future (will), basic modals (can/must/should)\\n\"\n",
    "    f\"- Connectors: and, but, because, so, when, if, then\\n\"\n",
    "    f\"- Style: Personal, concrete, everyday language\\n\\n\"\n",
    "    f\"STRICT LEVEL CONTROL:\\n\"\n",
    "    f\"- Above A1: Include personal experiences, feelings, plans, time references\\n\"\n",
    "    f\"- Below B1: No present perfect, passive voice, or complex connectors (however, although, despite)\\n\"\n",
    "    f\"- Below B1: No abstract concepts without concrete explanation\\n\\n\"\n",
    "    f\"TRANSFORMATION PROCESS:\\n\"\n",
    "    f\"1. Identify all key information and meaning\\n\"\n",
    "    f\"2. Break complex sentences into simple A2 structures\\n\"\n",
    "    f\"3. Replace advanced vocabulary with A2 equivalents\\n\"\n",
    "    f\"4. Convert complex grammar to simple A2 patterns\\n\"\n",
    "    f\"5. Verify all original meaning is preserved\\n\\n\"\n",
    "    f\"CRITICAL: Do not omit, summarize, or change any information. Only change HOW it's expressed.\\n\\n\"\n",
    "    f\"Return only the simplified text, with no explanations.\\n\\n\"\n",
    "    f\"Text to simplify:\\n\"\n",
    "    f\"\\\"\\\"\\\"\\n{original_text}\\n\\\"\\\"\\\"\\n\"\n",
    ")\n",
    "    if target_level == 'B1':\n",
    "        prompt = (\n",
    "    f\"You are an expert CEFR B1 text simplification specialist with deep understanding of automatic language assessment systems.\\n\\n\"\n",
    "    f\"OBJECTIVE: Transform this text to precise B1 level while preserving all original meaning and information.\\n\\n\"\n",
    "    f\"B1 LANGUAGE REQUIREMENTS:\\n\"\n",
    "    f\"- Vocabulary: 2000-3000 most common English words, avoid academic/formal terms\\n\"\n",
    "    f\"- Sentences: 15-22 words, can connect 2 related ideas with clear logic\\n\"\n",
    "    f\"- Grammar: Present perfect (have/has done), simple passive (is/was done), basic conditionals (if...will/would), modals (should, might, could, would)\\n\"\n",
    "    f\"- Connectors: however, although, while, since, unless, because, so that, even though\\n\"\n",
    "    f\"- Style: Clear intermediate language that shows reasoning and personal opinions\\n\\n\"\n",
    "\n",
    "    f\"STRICT LEVEL CONTROL:\\n\"\n",
    "    f\"- Above A2: Include abstract concepts with simple explanation, cause-effect relationships, personal opinions with basic justification, intermediate grammar patterns\\n\"\n",
    "    f\"- Below B2: No academic/formal vocabulary (facilitate→help, demonstrate→show, utilize→use), no complex conditional structures, no sophisticated argumentation, no specialized terminology without explanation\\n\"\n",
    "    f\"- PRECISE B1 TARGET: Intermediate complexity using everyday vocabulary - never oversimplify to A2, never undersimplify leaving B2+ elements\\n\\n\"\n",
    "\n",
    "    f\"CRITICAL B1 DIFFERENTIATORS:\\n\"\n",
    "    f\"- From A2: Can handle abstract ideas but explains them simply using common words\\n\"\n",
    "    f\"- From B2: Uses everyday vocabulary even for complex concepts, avoids formal/academic tone\\n\"\n",
    "    f\"- B1 signature: Connects ideas logically but with simple language patterns\\n\\n\"\n",
    "\n",
    "    f\"TRANSFORMATION PROCESS:\\n\"\n",
    "    f\"1. Identify all key information and meaning\\n\"\n",
    "    f\"2. Scan for B2+ vocabulary and replace with B1 common equivalents\\n\"\n",
    "    f\"3. Convert complex sentences to B1 structures (maximum 2 clauses per sentence)\\n\"\n",
    "    f\"4. Add simple explanations for any remaining complex concepts\\n\"\n",
    "    f\"5. Include 2-3 B1 grammar markers per paragraph naturally\\n\"\n",
    "    f\"6. Verify consistent B1 complexity throughout - no A2 oversimplification, no B2+ elements remaining\\n\\n\"\n",
    "\n",
    "    f\"CRITICAL: Do not omit, summarize, or change any information. Only change HOW it's expressed to match B1 patterns that automatic CEFR classifiers consistently recognize as B1 level.\\n\\n\"\n",
    "\n",
    "    f\"Return only the simplified text, with no explanations.\\n\\n\"\n",
    "\n",
    "    f\"Text to simplify:\\n\"\n",
    "    f\"\\\"\\\"\\\"\\n{original_text}\\n\\\"\\\"\\\"\\n\"\n",
    "    )\n",
    "    print(f\"Processing: {text_id}\")\n",
    "    simplified_text = gpt(prompt)\n",
    "    all_simplifications.append(simplified_text)\n",
    "    text_ids.append(text_id)\n",
    "\n",
    "with open('test_results/gpt_4_turbo_one_try.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for text_id, simplified_text in zip(text_ids, all_simplifications):\n",
    "        item = {\n",
    "            \"text_id\": text_id,\n",
    "            \"simplified\": simplified_text\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ],
   "id": "e4a8d789e57b627f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 21-a2\n",
      "Processing: 22-a2\n",
      "Processing: 23-a2\n",
      "Processing: 24-a2\n",
      "Processing: 25-a2\n",
      "Processing: 26-a2\n",
      "Processing: 27-a2\n",
      "Processing: 28-a2\n",
      "Processing: 29-a2\n",
      "Processing: 30-a2\n",
      "Processing: 31-a2\n",
      "Processing: 32-a2\n",
      "Processing: 33-a2\n",
      "Processing: 34-a2\n",
      "Processing: 35-a2\n",
      "Processing: 36-a2\n",
      "Processing: 37-a2\n",
      "Processing: 38-a2\n",
      "Processing: 39-a2\n",
      "Processing: 40-a2\n",
      "Processing: 41-a2\n",
      "Processing: 42-a2\n",
      "Processing: 43-a2\n",
      "Processing: 44-a2\n",
      "Processing: 45-a2\n",
      "Processing: 46-a2\n",
      "Processing: 47-a2\n",
      "Processing: 48-a2\n",
      "Processing: 49-a2\n",
      "Processing: 50-a2\n",
      "Processing: 51-a2\n",
      "Processing: 52-a2\n",
      "Processing: 53-a2\n",
      "Processing: 54-a2\n",
      "Processing: 55-a2\n",
      "Processing: 56-a2\n",
      "Processing: 57-a2\n",
      "Processing: 58-a2\n",
      "Processing: 59-a2\n",
      "Processing: 60-a2\n",
      "Processing: 61-a2\n",
      "Processing: 62-a2\n",
      "Processing: 63-a2\n",
      "Processing: 64-a2\n",
      "Processing: 65-a2\n",
      "Processing: 66-a2\n",
      "Processing: 67-a2\n",
      "Processing: 68-a2\n",
      "Processing: 69-a2\n",
      "Processing: 70-a2\n",
      "Processing: 71-a2\n",
      "Processing: 72-a2\n",
      "Processing: 73-a2\n",
      "Processing: 74-a2\n",
      "Processing: 75-a2\n",
      "Processing: 76-a2\n",
      "Processing: 77-a2\n",
      "Processing: 78-a2\n",
      "Processing: 79-a2\n",
      "Processing: 80-a2\n",
      "Processing: 81-a2\n",
      "Processing: 82-a2\n",
      "Processing: 83-a2\n",
      "Processing: 84-a2\n",
      "Processing: 85-a2\n",
      "Processing: 86-a2\n",
      "Processing: 87-a2\n",
      "Processing: 88-a2\n",
      "Processing: 89-a2\n",
      "Processing: 90-a2\n",
      "Processing: 91-a2\n",
      "Processing: 92-a2\n",
      "Processing: 93-a2\n",
      "Processing: 94-a2\n",
      "Processing: 95-a2\n",
      "Processing: 96-a2\n",
      "Processing: 97-a2\n",
      "Processing: 98-a2\n",
      "Processing: 99-a2\n",
      "Processing: 100-a2\n",
      "Processing: 101-a2\n",
      "Processing: 102-a2\n",
      "Processing: 103-a2\n",
      "Processing: 104-a2\n",
      "Processing: 105-a2\n",
      "Processing: 106-a2\n",
      "Processing: 107-a2\n",
      "Processing: 108-a2\n",
      "Processing: 109-a2\n",
      "Processing: 110-a2\n",
      "Processing: 111-a2\n",
      "Processing: 112-a2\n",
      "Processing: 113-a2\n",
      "Processing: 114-a2\n",
      "Processing: 115-a2\n",
      "Processing: 116-a2\n",
      "Processing: 117-a2\n",
      "Processing: 118-a2\n",
      "Processing: 119-a2\n",
      "Processing: 120-a2\n",
      "Processing: 21-b1\n",
      "Processing: 22-b1\n",
      "Processing: 23-b1\n",
      "Processing: 24-b1\n",
      "Processing: 25-b1\n",
      "Processing: 26-b1\n",
      "Processing: 27-b1\n",
      "Processing: 28-b1\n",
      "Processing: 29-b1\n",
      "Processing: 30-b1\n",
      "Processing: 31-b1\n",
      "Processing: 32-b1\n",
      "Processing: 33-b1\n",
      "Processing: 34-b1\n",
      "Processing: 35-b1\n",
      "Processing: 36-b1\n",
      "Processing: 37-b1\n",
      "Processing: 38-b1\n",
      "Processing: 39-b1\n",
      "Processing: 40-b1\n",
      "Processing: 41-b1\n",
      "Processing: 42-b1\n",
      "Processing: 43-b1\n",
      "Processing: 44-b1\n",
      "Processing: 45-b1\n",
      "Processing: 46-b1\n",
      "Processing: 47-b1\n",
      "Processing: 48-b1\n",
      "Processing: 49-b1\n",
      "Processing: 50-b1\n",
      "Processing: 51-b1\n",
      "Processing: 52-b1\n",
      "Processing: 53-b1\n",
      "Processing: 54-b1\n",
      "Processing: 55-b1\n",
      "Processing: 56-b1\n",
      "Processing: 57-b1\n",
      "Processing: 58-b1\n",
      "Processing: 59-b1\n",
      "Processing: 60-b1\n",
      "Processing: 61-b1\n",
      "Processing: 62-b1\n",
      "Processing: 63-b1\n",
      "Processing: 64-b1\n",
      "Processing: 65-b1\n",
      "Processing: 66-b1\n",
      "Processing: 67-b1\n",
      "Processing: 68-b1\n",
      "Processing: 69-b1\n",
      "Processing: 70-b1\n",
      "Processing: 71-b1\n",
      "Processing: 72-b1\n",
      "Processing: 73-b1\n",
      "Processing: 74-b1\n",
      "Processing: 75-b1\n",
      "Processing: 76-b1\n",
      "Processing: 77-b1\n",
      "Processing: 78-b1\n",
      "Processing: 79-b1\n",
      "Processing: 80-b1\n",
      "Processing: 81-b1\n",
      "Processing: 82-b1\n",
      "Processing: 83-b1\n",
      "Processing: 84-b1\n",
      "Processing: 85-b1\n",
      "Processing: 86-b1\n",
      "Processing: 87-b1\n",
      "Processing: 88-b1\n",
      "Processing: 89-b1\n",
      "Processing: 90-b1\n",
      "Processing: 91-b1\n",
      "Processing: 92-b1\n",
      "Processing: 93-b1\n",
      "Processing: 94-b1\n",
      "Processing: 95-b1\n",
      "Processing: 96-b1\n",
      "Processing: 97-b1\n",
      "Processing: 98-b1\n",
      "Processing: 99-b1\n",
      "Processing: 100-b1\n",
      "Processing: 101-b1\n",
      "Processing: 102-b1\n",
      "Processing: 103-b1\n",
      "Processing: 104-b1\n",
      "Processing: 105-b1\n",
      "Processing: 106-b1\n",
      "Processing: 107-b1\n",
      "Processing: 108-b1\n",
      "Processing: 109-b1\n",
      "Processing: 110-b1\n",
      "Processing: 111-b1\n",
      "Processing: 112-b1\n",
      "Processing: 113-b1\n",
      "Processing: 114-b1\n",
      "Processing: 115-b1\n",
      "Processing: 116-b1\n",
      "Processing: 117-b1\n",
      "Processing: 118-b1\n",
      "Processing: 119-b1\n",
      "Processing: 120-b1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, root_mean_squared_error\n",
    "from transformers import pipeline\n",
    "\n",
    "CEFR_LABELS = ['A1','A2','B1','B2','C1','C2']\n",
    "LABEL2IDX   = {label: idx for idx, label in enumerate(CEFR_LABELS)}\n",
    "\n",
    "cefr_labeler1 = pipeline(task=\"text-classification\",model=\"AbdullahBarayan/ModernBERT-base-doc_en-Cefr\" )\n",
    "cefr_labeler2 = pipeline(task=\"text-classification\",model=\"AbdullahBarayan/ModernBERT-base-doc_sent_en-Cefr\")\n",
    "cefr_labeler3 = pipeline(task=\"text-classification\",model=\"AbdullahBarayan/ModernBERT-base-reference_AllLang2-Cefr2\")\n",
    "\n",
    "\n",
    "def get_cefr_labels(simplifications: list, models=[cefr_labeler1,cefr_labeler2,cefr_labeler3]):\n",
    "  cefr_labels = []\n",
    "  for simplification in simplifications:\n",
    "    top_preds = (model(simplification)[0] for model in models)\n",
    "    best = max(top_preds, key=lambda d: d[\"score\"])\n",
    "    cefr_labels.append(best[\"label\"])\n",
    "  return cefr_labels\n",
    "\n",
    "def get_cefr_compliance_score(simplifications: list, reference_levels: list, models=[cefr_labeler1,cefr_labeler2,cefr_labeler3]):\n",
    "    assert len(simplifications) == len(reference_levels), \"The number of simplifications is different of the number of reference_levels.\"\n",
    "\n",
    "    predicted_labels = get_cefr_labels(simplifications=simplifications, models=models)\n",
    "    f1 = f1_score(reference_levels, predicted_labels, average='weighted')\n",
    "\n",
    "    true_idx = np.array([LABEL2IDX[l] for l in reference_levels])\n",
    "    pred_idx = np.array([LABEL2IDX[l] for l in predicted_labels])\n",
    "\n",
    "    adj_acc = (np.abs(true_idx - pred_idx) <= 1).mean()\n",
    "    rmse = root_mean_squared_error(true_idx, pred_idx)\n",
    "\n",
    "    return {'weighted_f1': round(f1,4),\n",
    "            'adj_accuracy': round(adj_acc,4),\n",
    "            'rmse': round(rmse,4)}\n",
    "\n",
    "def evaluate_single_text_cefr(text, target_level, models=[cefr_labeler1,cefr_labeler2,cefr_labeler3]):\n",
    "    try:\n",
    "        simplifications = [text]\n",
    "        reference_levels = [target_level.upper()]\n",
    "\n",
    "        predicted_labels = get_cefr_labels(simplifications=simplifications, models=models)\n",
    "\n",
    "        if predicted_labels and len(predicted_labels) > 0 and predicted_labels[0] is not None:\n",
    "            predicted_level = predicted_labels[0]\n",
    "\n",
    "            f1 = f1_score(reference_levels, predicted_labels, average='weighted')\n",
    "\n",
    "            true_idx = np.array([LABEL2IDX[l] for l in reference_levels])\n",
    "            pred_idx = np.array([LABEL2IDX[l] for l in predicted_labels])\n",
    "\n",
    "            adj_acc = (np.abs(true_idx - pred_idx) <= 1).mean()\n",
    "            rmse = root_mean_squared_error(true_idx, pred_idx)\n",
    "\n",
    "            metrics = {\n",
    "                'weighted_f1': round(f1, 4),\n",
    "                'adj_accuracy': round(adj_acc, 4),\n",
    "                'rmse': round(rmse, 4)\n",
    "            }\n",
    "            return predicted_level, metrics\n",
    "        else:\n",
    "            return None, {'weighted_f1': 0.0, 'adj_accuracy': 0.0, 'rmse': 0.0}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        return None, {'weighted_f1': 0.0, 'adj_accuracy': 0.0, 'rmse': 0.0}\n",
    "def iterative_cefr_simplification(df, max_iterations=5):\n",
    "\n",
    "    def get_simplification_prompt(original_text, target_level, current_level=None, iteration=0):\n",
    "\n",
    "        base_feedback = \"\"\n",
    "        if current_level and iteration > 0:\n",
    "            if current_level != target_level.upper():\n",
    "                base_feedback = f\"\\nCURRENT ISSUE: The previous version was classified as {current_level}, but we need {target_level.upper()} level. \"\n",
    "\n",
    "                if current_level > target_level.upper():\n",
    "                    base_feedback += \"The text is TOO COMPLEX. Simplify more aggressively.\"\n",
    "                else:\n",
    "                    base_feedback += \"The text is TOO SIMPLE. Add more complexity while staying at target level.\"\n",
    "\n",
    "        if target_level.lower() == 'a2':\n",
    "            return f\"\"\"You are a language teacher simplifying texts to A2 CEFR level.\n",
    "\n",
    "OBJECTIVE: Transform this text to A2 level while preserving all original meaning and information.\n",
    "{base_feedback}\n",
    "\n",
    "A2 LANGUAGE REQUIREMENTS:\n",
    "- Vocabulary: Most common 1500 English words only\n",
    "- Sentences: 8-12 words, one clear idea per sentence\n",
    "- Grammar: Simple present/past, basic future (will), basic modals (can/must/should)\n",
    "- Connectors: and, but, because, so, when, if, then\n",
    "- Style: Personal, concrete, everyday language\n",
    "\n",
    "STRICT LEVEL CONTROL:\n",
    "- Above A1: Include personal experiences, feelings, plans, time references\n",
    "- Below B1: No present perfect, passive voice, or complex connectors (however, although, despite)\n",
    "- Below B1: No abstract concepts without concrete explanation\n",
    "\n",
    "TRANSFORMATION PROCESS:\n",
    "1. Identify all key information and meaning\n",
    "2. Break complex sentences into simple A2 structures\n",
    "3. Replace advanced vocabulary with A2 equivalents\n",
    "4. Convert complex grammar to simple A2 patterns\n",
    "5. Verify all original meaning is preserved\n",
    "\n",
    "CRITICAL: Do not omit, summarize, or change any information. Only change HOW it's expressed.\n",
    "\n",
    "Return only the simplified text. Do not include any other comments, notes, or additional information.\n",
    "\n",
    "Text to simplify:\n",
    "\\\"\\\"\\\"\n",
    "{original_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        elif target_level.lower() == 'b1':\n",
    "            return f\"\"\"You are an expert CEFR B1 text simplification specialist with deep understanding of automatic language assessment systems.\n",
    "\n",
    "OBJECTIVE: Transform this text to precise B1 level while preserving all original meaning and information.\n",
    "{base_feedback}\n",
    "\n",
    "B1 LANGUAGE REQUIREMENTS:\n",
    "- Vocabulary: 2000-3000 most common English words, avoid academic/formal terms\n",
    "- Sentences: 15-22 words, can connect 2 related ideas with clear logic\n",
    "- Grammar: Present perfect (have/has done), simple passive (is/was done), basic conditionals (if...will/would), modals (should, might, could, would)\n",
    "- Connectors: however, although, while, since, unless, because, so that, even though\n",
    "- Style: Clear intermediate language that shows reasoning and personal opinions\n",
    "\n",
    "STRICT LEVEL CONTROL:\n",
    "- Above A2: Include abstract concepts with simple explanation, cause-effect relationships, personal opinions with basic justification, intermediate grammar patterns\n",
    "- Below B2: No academic/formal vocabulary (facilitate→help, demonstrate→show, utilize→use), no complex conditional structures, no sophisticated argumentation, no specialized terminology without explanation\n",
    "- PRECISE B1 TARGET: Intermediate complexity using everyday vocabulary - never oversimplify to A2, never undersimplify leaving B2+ elements\n",
    "\n",
    "CRITICAL B1 DIFFERENTIATORS:\n",
    "- From A2: Can handle abstract ideas but explains them simply using common words\n",
    "- From B2: Uses everyday vocabulary even for complex concepts, avoids formal/academic tone\n",
    "- B1 signature: Connects ideas logically but with simple language patterns\n",
    "\n",
    "TRANSFORMATION PROCESS:\n",
    "1. Identify all key information and meaning\n",
    "2. Scan for B2+ vocabulary and replace with B1 common equivalents\n",
    "3. Convert complex sentences to B1 structures (maximum 2 clauses per sentence)\n",
    "4. Add simple explanations for any remaining complex concepts\n",
    "5. Include 2-3 B1 grammar markers per paragraph naturally\n",
    "6. Verify consistent B1 complexity throughout - no A2 oversimplification, no B2+ elements remaining\n",
    "\n",
    "CRITICAL: Do not omit, summarize, or change any information. Only change HOW it's expressed to match B1 patterns that automatic CEFR classifiers consistently recognize as B1 level.\n",
    "\n",
    "Return only the simplified text. Do not include any other comments, notes, or additional information.\n",
    "\n",
    "Text to simplify:\n",
    "\\\"\\\"\\\"\n",
    "{original_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    results = []\n",
    "    best_versions = []\n",
    "    all_simplifications = []\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "        target_level = df['target_cefr'][idx].lower()\n",
    "        original_text = df['original'][idx]\n",
    "        text_id = df['text_id'][idx]\n",
    "\n",
    "        print(f\"\\n=== Processing {text_id} (Target: {target_level.upper()}) ===\")\n",
    "\n",
    "        current_text = original_text\n",
    "        simplified_text = None\n",
    "        iteration_history = []\n",
    "\n",
    "        best_iteration = None\n",
    "\n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"\\nIteration {iteration + 1}/{max_iterations}\")\n",
    "\n",
    "            if iteration == 0:\n",
    "                prompt = get_simplification_prompt(current_text, target_level)\n",
    "            else:\n",
    "                last_level = iteration_history[-1]['predicted_level']\n",
    "                prompt = get_simplification_prompt(current_text, target_level, last_level, iteration)\n",
    "\n",
    "            simplified_text = gpt(prompt)\n",
    "\n",
    "            predicted_level, metrics = evaluate_single_text_cefr(simplified_text, target_level)\n",
    "\n",
    "            iteration_info = {\n",
    "                'iteration': iteration + 1,\n",
    "                'text': simplified_text,\n",
    "                'predicted_level': predicted_level,\n",
    "                'weighted_f1': metrics['weighted_f1'],\n",
    "                'adj_accuracy': metrics['adj_accuracy'],\n",
    "                'rmse': metrics['rmse'],\n",
    "                'target_achieved': predicted_level == target_level.upper() if predicted_level else False\n",
    "            }\n",
    "\n",
    "            iteration_history.append(iteration_info)\n",
    "\n",
    "            if best_iteration is None or metrics['weighted_f1'] > best_iteration['weighted_f1']:\n",
    "                best_iteration = iteration_info.copy()\n",
    "\n",
    "            all_simplifications.append({\n",
    "                'text_id': text_id,\n",
    "                'iteration': iteration + 1,\n",
    "                'target_level': target_level,\n",
    "                'predicted_level': predicted_level,\n",
    "                'weighted_f1': metrics['weighted_f1'],\n",
    "                'adj_accuracy': metrics['adj_accuracy'],\n",
    "                'rmse': metrics['rmse'],\n",
    "                'simplified_text': simplified_text,\n",
    "                'original_text': original_text if iteration == 0 else None\n",
    "            })\n",
    "\n",
    "            print(f\"Prediction: {predicted_level}\")\n",
    "            print(f\"F1: {metrics['weighted_f1']}, Adj Acc: {metrics['adj_accuracy']}, RMSE: {metrics['rmse']}\")\n",
    "            print(f\"Target achieved: {'Yes' if iteration_info['target_achieved'] else 'No'}\")\n",
    "\n",
    "            if iteration_info['target_achieved']:\n",
    "                print(f\"Target {target_level.upper()} achieved at iteration {iteration + 1}!\")\n",
    "                break\n",
    "\n",
    "            current_text = simplified_text\n",
    "\n",
    "        final_result = {\n",
    "            'text_id': text_id,\n",
    "            'original': original_text,\n",
    "            'target_level': target_level,\n",
    "            'final_simplified': simplified_text,\n",
    "            'iterations_used': len(iteration_history),\n",
    "            'final_predicted_level': iteration_history[-1]['predicted_level'],\n",
    "            'final_weighted_f1': iteration_history[-1]['weighted_f1'],\n",
    "            'final_adj_accuracy': iteration_history[-1]['adj_accuracy'],\n",
    "            'final_rmse': iteration_history[-1]['rmse'],\n",
    "            'target_achieved': iteration_history[-1]['target_achieved'],\n",
    "            'iteration_history': iteration_history\n",
    "        }\n",
    "\n",
    "        results.append(final_result)\n",
    "\n",
    "        best_versions.append({\n",
    "            'text_id': text_id,\n",
    "            'simplified': best_iteration['text']\n",
    "        })\n",
    "\n",
    "        if final_result['target_achieved']:\n",
    "            print(f\"Success: {text_id} -> {target_level.upper()} in {len(iteration_history)} iterations\")\n",
    "        else:\n",
    "            print(f\"Failed: {text_id} -> {final_result['final_predicted_level']} instead of {target_level.upper()}\")\n",
    "\n",
    "    return results, all_simplifications, best_versions\n",
    "\n",
    "def analyze_iterative_results(results):\n",
    "\n",
    "    total_texts = len(results)\n",
    "    successful_texts = sum(1 for r in results if r['target_achieved'])\n",
    "    success_rate = successful_texts / total_texts * 100\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ITERATIVE SIMPLIFICATION RESULTS ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Processed texts: {total_texts}\")\n",
    "    print(f\"Successful texts: {successful_texts}\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    iteration_stats = {}\n",
    "    for result in results:\n",
    "        iterations_used = result['iterations_used']\n",
    "        if iterations_used not in iteration_stats:\n",
    "            iteration_stats[iterations_used] = {'total': 0, 'successful': 0}\n",
    "\n",
    "        iteration_stats[iterations_used]['total'] += 1\n",
    "        if result['target_achieved']:\n",
    "            iteration_stats[iterations_used]['successful'] += 1\n",
    "\n",
    "    print(f\"\\nDistribution by iterations:\")\n",
    "    for iterations, stats in sorted(iteration_stats.items()):\n",
    "        success_rate_iter = stats['successful'] / stats['total'] * 100\n",
    "        print(f\"  {iterations} iterations: {stats['successful']}/{stats['total']} ({success_rate_iter:.1f}% success)\")\n",
    "\n",
    "    level_stats = {}\n",
    "    for result in results:\n",
    "        target = result['target_level']\n",
    "        if target not in level_stats:\n",
    "            level_stats[target] = {'total': 0, 'successful': 0}\n",
    "\n",
    "        level_stats[target]['total'] += 1\n",
    "        if result['target_achieved']:\n",
    "            level_stats[target]['successful'] += 1\n",
    "\n",
    "    print(f\"\\nPerformance by levels:\")\n",
    "    for level, stats in sorted(level_stats.items()):\n",
    "        success_rate_level = stats['successful'] / stats['total'] * 100\n",
    "        print(f\"  {level.upper()}: {stats['successful']}/{stats['total']} ({success_rate_level:.1f}% success)\")\n",
    "\n",
    "    multi_iteration_texts = []\n",
    "    for result in results:\n",
    "        if result['iterations_used'] > 1:\n",
    "            multi_iteration_texts.append({\n",
    "                'text_id': result['text_id'],\n",
    "                'iterations': result['iterations_used'],\n",
    "                'final_level': result['final_predicted_level'],\n",
    "                'target_achieved': result['target_achieved']\n",
    "            })\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TEXTS WITH MULTIPLE ITERATIONS ({len(multi_iteration_texts)} texts):\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for text in multi_iteration_texts:\n",
    "        status = \"✓\" if text['target_achieved'] else \"✗\"\n",
    "        print(f\"{status} {text['text_id']}: {text['iterations']} iterations (final: {text['final_level']})\")\n",
    "\n",
    "    return {\n",
    "        'total_texts': total_texts,\n",
    "        'successful_texts': successful_texts,\n",
    "        'success_rate': success_rate,\n",
    "        'iteration_stats': iteration_stats,\n",
    "        'level_stats': level_stats,\n",
    "        'multi_iteration_texts': multi_iteration_texts\n",
    "    }\n",
    "\n",
    "def save_all_simplifications(all_simplifications, best_versions, filename='test_results/iterative_gpt-4-1106.csv'):\n",
    "\n",
    "    import json\n",
    "    with open('tsar2025sharedtask_evaluation/submissions/test_results/iterative_gpt-4-1106.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in best_versions:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(\"Best versions saved to 'best_versions.jsonl'\")\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df_simplifications = pd.DataFrame(all_simplifications)\n",
    "\n",
    "    df_simplifications.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nToate simplificările au fost salvate în '{filename}'\")\n",
    "\n",
    "    import json\n",
    "    json_filename = filename.replace('.csv', '.json')\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_simplifications, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Format JSON salvat în '{json_filename}'\")\n",
    "\n",
    "def main():\n",
    "    iterative_results, all_simplifications, best_versions = iterative_cefr_simplification(df, max_iterations=5)\n",
    "\n",
    "\n",
    "    if iterative_results:\n",
    "        analysis = analyze_iterative_results(iterative_results)\n",
    "\n",
    "        save_all_simplifications(all_simplifications, best_versions)\n",
    "\n",
    "        final_texts = [r['final_simplified'] for r in iterative_results]\n",
    "        reference_levels = [r['target_level'].upper() for r in iterative_results]\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"DETAILED FINAL EVALUATION\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        compliance_scores = get_cefr_compliance_score(final_texts, reference_levels)\n",
    "        print(f\"Weighted F1: {compliance_scores['weighted_f1']}\")\n",
    "        print(f\"Adjacent Accuracy: {compliance_scores['adj_accuracy']}\")\n",
    "        print(f\"RMSE: {compliance_scores['rmse']}\")\n",
    "\n",
    "        return iterative_results, analysis, compliance_scores, all_simplifications, best_versions\n",
    "\n",
    "    return None\n",
    "\n",
    "iterative_results, analysis, compliance_scores, all_simplifications, best_versions = main()"
   ],
   "id": "ae443d72cb9c05f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
